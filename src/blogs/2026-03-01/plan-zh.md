# Content Plan: 一个学徒厨师如何学会做菜 — 用厨房解释 ChatGPT 的全部算法

## Meta
- **灵感来源:** Andrej Karpathy — [MicroGPT](https://karpathy.github.io/2026/02/12/microgpt/)
- **参考归档:** `src/reading/2026-03-01-karpathy-microgpt.md`
- **核心定位:** Karpathy 把 GPT 简化到 200 行，但普通人还是读不懂。我们再降一层——用一个完整的厨房故事，让**任何人**都能懂。
- **统一比喻:** 🍳 一个学徒厨师在一间有 4192 个调料旋钮的厨房里学做菜
- **目标读者:** 对 AI 好奇但没有编程/数学背景的人
- **语言:** 中英双版本（中文先写）
- **风格:** 一个连贯的故事 + 零公式

---

## 比喻映射表

| GPT 概念 | 厨房比喻 |
|----------|---------|
| 模型（Model） | 学徒厨师小明 |
| 参数（Parameters） | 厨房里 4192 个调料旋钮（盐多少、火多大、酱油几滴…） |
| 数据集（Dataset） | 32000 道师傅做过的菜（学习用的菜谱照片，不是文字菜谱） |
| Token | omakase 里的每一道（course）—— 顺序很重要 |
| Tokenizer | 把一套完整的 omakase 拆成"一道一道"的标准化方式 |
| BOS Token | 服务铃 🛎️ —— "新的一套 omakase 开始了" / "这套结束了" |
| Embedding | 每道菜的"风味档案卡"（不是菜名，是一组描述性数字：咸度、鲜度、口感…） |
| Position Embedding | 上菜顺序标签 —— "这是第 3 道" |
| Attention | 主厨回头翻看之前几道的笔记 —— "第 1 道上了刺身，第 3 道该不该上烤物？" |
| Query / Key / Value | Q="我现在需要什么味道？" K="我之前提供了什么味道？" V="如果选中我，这是我的配方" |
| Multi-head Attention | 4 个副厨同时回头看，各自关注不同维度（一个看咸淡，一个看口感，一个看温度，一个看颜色） |
| KV Cache | 副厨的笔记本 —— 记着之前每道菜的信息，不用重新尝 |
| MLP | 后厨加工间 —— 把收集来的信息"炒一炒"，提炼出新的理解 |
| ReLU | 加工间的品控员 —— 好的味道通过，坏的直接倒掉（负数→零） |
| Residual Connection | 每次加工后，留一份原味样本混回去 —— 防止反复加工后味道完全走样 |
| RMSNorm | 每道工序前先"校准味觉" —— 确保舌头灵敏度一致 |
| Loss | 食客打分 —— 离满分差多少？（水桶里的水） |
| Softmax | 把所有可能的下一道菜排成概率 —— "70% 该上虾，20% 上豆腐，10% 上牛肉" |
| Backpropagation | 食客说"太咸了"→ 厨房从最后一步往回追 → "是哪个环节、哪个旋钮让它变咸的？" |
| Gradient | 每个旋钮的"影响力报告" —— "盐旋钮对咸度的影响 = 4，糖旋钮的影响 = 0.5" |
| Chain Rule | 追踪接力 —— "酱油影响了汤底 ×3，汤底影响了总味道 ×2，所以酱油对总味道 = ×6" |
| Adam Optimizer | 一个有记忆的老师傅 —— 记得最近几次的调整方向（不会来回拉锯），对灵敏的旋钮轻调、迟钝的重调 |
| Learning Rate | 师傅的手劲 —— 开始大胆调，越练越谨慎（微调） |
| Training Loop | 一套 omakase 一套地练：做→尝→追→调，重复 1000 次 |
| Inference | 出师了！师傅不在，自己设计 omakase 给客人吃 |
| Temperature | 厨师的冒险程度 —— 低温=严格按经验、高温=大胆创新 |
| Hallucination | 编了一道听起来合理但不存在的菜 —— "松露味增拉面"（plausible but fake） |
| MicroGPT → ChatGPT | 路边摊 → 米其林三星 —— 同样的烹饪原理，规模天差地别 |

---

## 文章大纲

### 开头 Hook

> 想象你走进一间 omakase 厨房。
>
> 厨房里有一个叫小明的学徒，一面墙上有 4192 个调料旋钮，还有 32000 套师傅做过的 omakase 的完整记录——每套几道菜、什么顺序上的。
>
> 小明的任务：只看这些记录，把旋钮调到对的位置，学会设计自己的 omakase。
>
> 没人教他规则。没人告诉他"刺身要在烤物前面"。他只能一遍一遍地试，让食客打分，然后从分数里倒推哪个旋钮该往哪边转。
>
> 这就是 ChatGPT 的全部原理。不是比喻——是字面意思。
>
> Andrej Karpathy 用 200 行代码证明了这一点。今天我用一间厨房帮你彻底搞懂。

---

### 第一章：食材 — 32000 套 Omakase 📋

小明面前有师傅留下的作品集：32000 套 omakase 的完整记录（不是菜谱！只有成品照片和上菜顺序）。

- "emma" = 一套 4 道的 omakase（e → m → m → a）
- "sophia" = 一套 6 道的（s → o → p → h → i → a）
- 他只能看成品和顺序，不知道怎么做
- 目标：看够了之后，自己设计新的 omakase——让食客觉得"像那么回事"

**为什么是 omakase？** 因为顺序是灵魂。同样的 4 道菜，先上刺身再上烤物 vs 反过来，体验完全不同。GPT 也一样——"emma" 和 "amme" 是完全不同的序列。

**延伸：** ChatGPT 的作品集 = 整个互联网。几万亿套"omakase"。

**关键感受：** 模型不是背菜谱，是吃了三万套 omakase 后形成了"直觉"——什么该接什么，什么搭配起来"顺"。

---

### 第二章：拆解 — 一道一道地学 🥢

一套 omakase 太复杂，没法整体学。得拆成一道一道。

- "emma" → [🛎️, e, m, m, a, 🛎️]
- 服务铃 🛎️ = "新套餐开始" / "套餐结束"
- 每一道菜（course）就是一个 token

**但神经网络不认识菜名，只认识数字。** 所以需要一个"编号系统"：

- a=0号菜, b=1号菜, c=2号菜 ... z=25号菜, 🛎️=26号
- "emma" 这套 omakase → [26, 4, 12, 12, 0, 26]

**延伸：** ChatGPT 的编号系统更聪明——常见的菜品组合（比如"刺身拼盘"）只占一个编号，更高效。

**风味档案卡：** 光有编号还不够。编号只是名字，厨房需要知道每道菜的"性格"。所以每个编号对应一张风味档案卡——16 个数字，描述这道菜的各种特性（咸度、温度、口感、浓郁度……）。这些数字一开始是随机的，训练中慢慢变得有意义。

---

### 第三章：厨房流水线 — 下一道该上什么？ 🍽️

小明要预测：**这套 omakase 的下一道应该上什么？**

每一道菜进入厨房后，经过三个工位：

#### 工位 1：副厨会议桌（Attention）👥

当前这道（比如第 3 道 "m"）被端到桌上。4 个副厨坐在桌边，每人看不同的维度：

- 副厨 A（管咸淡）回头翻笔记："第 1 道是清淡的，第 2 道是浓郁的"
- 副厨 B（管口感）也翻笔记："前面都是软的，该来点脆的了？"
- 每个副厨问（Query）："以我关注的维度，之前哪道菜跟我现在最相关？"
- 之前的每道菜回答（Key）："我是这种风味"
- 匹配度高的把详细配方（Value）传过来
- 4 个副厨的发现汇总在一起

**这是唯一一个"回头看"的环节。** 其余工位只看当前这道菜。

笔记本不用每次重写——之前的 Key 和 Value 存在本子上（KV Cache），新的一道只需要加一页。

#### 工位 2：后厨加工间（MLP）🔥

副厨会议的结论送进加工间：
- 先展开思路（扩大 4 倍）—— "所有可能的味道组合"
- 品控员（ReLU）把不靠谱的直接扔掉（负数 → 0）
- 再浓缩回来 —— 提炼出精华

**Attention 是"问同事"，MLP 是"自己想"。** GPT 就是交替执行：问问别人→自己想想→问问别人→自己想想。

#### 保险机制：留原味样本（Residual）

每次加工后，把原味混回去。像做酱汁时永远留一勺原汤——防止调过头。

#### 校准环节（RMSNorm）

每道工序前，先"漱口清零"。确保味觉灵敏度一致。不然吃了辣的再吃甜的，判断会出错。

#### 最终投票

加工完毕，对 27 种可能的下一道菜打分：
- "70% 该上 a，15% 上 e，8% 上 i..."
- 分数转概率（Softmax）= 把打分变成百分比

---

### 第四章：食客打分 — 你做得有多差？ 📝

小明做完一套 omakase，食客来打分。每道菜都要评：

- 如果小明 100% 确信下一道该上 "a"，而且确实是 "a" → 完美，0 分扣分
- 如果他只给了 "a" 10% 的概率 → 大扣分
- 如果他给了 0.1% → 扣到爆

**这个扣分就是 Loss。Loss 越低，厨艺越好。**

开局 loss = 3.3 = 27 种里随机猜 = 厨房新手，完全靠蒙。

---

### 第五章：追源 — 太咸了是谁的错？ 🔍（核心重头戏）

食客吃了一口："太咸了。"

小明不能一个一个试 4192 个旋钮。但他可以**从盘子往回追**：

#### 5.1 从盘子到厨房

"这口菜太咸 → 最后一步是装盘（没加盐）→ 上一步是加工间（加了酱油）→ 再上一步是副厨会议（参考了第 2 口的信息）→ 再上一步是调料旋钮 #347（酱油量）"

**这就是反向传播 —— 从结果追到原因。**

#### 5.2 追踪接力（Chain Rule）

每一步都知道一个简单的事：**"如果上游变了 1，我会变多少？"**

- 酱油旋钮拧大 1 → 汤底咸度 +3
- 汤底咸度 +1 → 最终味道 +2
- 所以酱油旋钮对最终味道的影响 = 3 × 2 = **6**

**沿着厨房流水线，一步一步相乘回去。** 这就是链式法则。不需要微积分，只需要沿路乘。

#### 5.3 六种基本操作

整个厨房只有六种加工手法，每种都知道怎么往回追：

| 手法 | 做什么 | 往回追时 |
|------|--------|---------|
| **混合**（加法） | 两种配料倒在一起 | 两边影响相同 |
| **调配**（乘法） | 两种配料互相增强 | A 的影响 = B 的量，反之亦然 |
| **浓缩**（乘方） | 味道加倍再加倍 | 越浓影响越猛 |
| **提味**（log） | 一点点就很明显 | 量越少越敏感 |
| **发酵**（exp） | 膨胀式增长 | 越多膨胀越快 |
| **品控**（ReLU） | 好的留、坏的倒 | 留下的追踪，倒掉的归零 |

**整个 ChatGPT 的厨房只用这六种手法。没有第七种。**

#### 5.4 举个例子

小明做了一道简单的菜：
- 食材 a=2（盐），b=3（糖）
- 第一步：混合 c = a × b = 6（盐糖调味）
- 第二步：最终味道 L = c + a = 8（调味+再补一点盐）

食客说味道偏了（L=8 太高），从盘子往回追：
- L = c + a → 混合（加法）→ c 和 a 的影响都是 1
- c = a × b → 调配（乘法）→ a 的影响 = b 的值 = 3

a 出现了两次（调配里 + 最终混合里），总影响 = 3 + 1 = **4**

意思：**盐（a）每多加 0.001，最终味道偏 0.004。** 这就是 gradient = 4。

现在小明知道了：盐的影响力是 4，糖是 2。盐要多调一点，糖少调一点。

---

### 第六章：师傅的练习法 — 1000 道菜 🔄

小明开始反复练习。每道菜做同样三件事：

1. **做菜**（正向）—— 按当前旋钮位置做一道菜
2. **追源**（反向）—— 食客打分后，从盘子追回每个旋钮的影响力
3. **调旋钮**（更新）—— 哪个旋钮让分变差就反方向微调

#### 老师傅 Adam（优化器）

不是每个旋钮都调一样多。Adam 师傅更聪明：

- **记忆**（Momentum）—— 记住最近几次的调整方向。如果连续 5 次都说"盐要减"，第 6 次就大胆减多一点。像滚下坡的球，越滚越有惯性。
- **因材施教**（Adaptive LR）—— 有的旋钮很灵敏（动一点味道变很多），就轻轻调。有的旋钮很迟钝（使劲拧才有效），就用力调。
- **手劲递减** —— 刚开始大胆调（反正离好吃很远），越接近好吃越小心微调。

#### 成长曲线

| 阶段 | Loss | 厨房里的表现 |
|------|------|------------|
| 第 1 套 | 3.3 | 闭眼乱配——甜品后面上生鱼片，毫无逻辑（27 道菜随机猜） |
| 第 100 套 | ~2.8 | 学会了"套餐通常以清淡开头、浓郁收尾" |
| 第 500 套 | ~2.5 | 学会了"味增汤和白饭总是一起出现""不会连上三道油炸" |
| 第 1000 套 | 2.37 | 能编出一套让食客信服的 omakase 了 |

**小明没有背任何菜谱规则。** 没人告诉他"油炸不能连着上"或"味增配白饭"。他只是通过"做→尝→追→调"的循环，旋钮自然落到了反映这些规律的位置。这就是"学会了直觉"。

---

### 第七章：出师 — 小明自己做菜 🌟

训练结束，旋钮锁定。现在不是练习了——食客来了。

1. 服务铃响 🛎️ = "设计一套新的 omakase"
2. 小明按当前旋钮做出第一道
3. 根据第一道的风味，决定第二道该上什么
4. 继续，直到自然做出结束信号 🛎️

**每一道都是即兴的，基于之前所有道的"记忆"。** 就像真正的 omakase 大厨——看食客吃前几道的反应，即兴调整后面的菜单。

#### 冒险旋钮（Temperature）

厨房门口有一个额外的旋钮，不影响厨艺，只影响**风格**：

- **Temperature = 0.1**（极度保守）→ 永远做最安全的选择 → "anna, emma, sarah" → 好吃但无聊
- **Temperature = 0.5**（略保守）→ "karia, yeran, liole" → 好吃且有新意
- **Temperature = 1.5**（大胆冒险）→ "zqxmf...?" → 可能惊艳，可能翻车
- **Temperature → 0**（完全不冒险）→ 每次做同一道菜

**ChatGPT 跟你对话时的温度大约 0.7 —— 有一点创意，但不会太离谱。**

#### 关于"编菜"（Hallucination）

小明做出了一道叫 "karia" 的菜。听起来像真菜，但菜单上没有。

他不是在撒谎。他只是按照学到的"味觉模式"自然做出来的——统计上说得通，但现实中不存在。

**ChatGPT 编造一个不存在的论文引用，跟小明编出 "karia" 是完全一样的机制。** 它不知道什么是"真"，只知道什么"吃起来对"。

---

### 第八章：从路边摊到米其林三星 🏆

小明学会了用 4192 个旋钮做出像样的名字。ChatGPT 呢？

| | 小明（路边摊） | ChatGPT（米其林三星） |
|---|---|---|
| 旋钮数 | 4,192 | 几千亿 |
| 学习菜品 | 32,000 个名字 | 万亿道"菜"（整个互联网） |
| 食材编号 | 27 个字母 | ~100,000 个词组 |
| 厨房设备 | 一个炉子（MacBook） | 几千口灶同时烧（GPU 集群） |
| 训练时间 | 1 分钟 | 几个月 |
| **做菜原理** | **完全一样** | **完全一样** |

#### 三星厨房的额外步骤

1. **换菜谱**（SFT）—— 小明先看名字学"语言感觉"，然后换成对话记录继续练。算法不变，只是学习材料换了。像一个厨师先学煎蛋，再学法餐。

2. **请美食评委**（RLHF）—— 小明做两道菜，评委选更好的那道。根据评委偏好调整旋钮。这就是 ChatGPT 变得"有礼貌"、"有帮助"的原因——不是规则，是评委训练出来的偏好。

**但核心永远是：做菜 → 打分 → 追源 → 调旋钮。**

---

### 结尾

> 下次有人跟你说 AI 很神秘、很恐怖、或者要取代人类时，记住这间厨房：
>
> 一个学徒，一面旋钮墙，三万道菜的照片。
>
> 他不懂"规则"。他只是做了一千遍，每次让食客打分，从分数里倒推该怎么调。
>
> 做久了，旋钮自然就落到了正确的位置。
>
> 这不是智能。这是统计上的直觉。
>
> 但这么简单的机制能产生这么强大的能力——这才是真正让人敬畏的地方。
>
> 200 行代码。六种手法。一间厨房。
>
> 这就是 ChatGPT 的全部。

---

## 产出规划

| 格式 | 内容 | 状态 |
|------|------|------|
| **Blog（完整版）** | 全 8 章，中英双版本，配厨房插图 | 待写 |
| **X Thread** | 10 条精简版，hook: "ChatGPT = 一间有 4192 个旋钮的厨房" | 待写 |
| **YouTube Script** | 5-8 分钟讲解版，配厨房动画 | 待定 |
| **Knowledge Short** | 60s 版本，一个核心场景 | 待定 |

## 配图需求
1. **厨房全景** — 小明站在旋钮墙前，面前是 32000 张菜品照片
2. **厨房流水线** — 风味档案卡桌 → 副厨会议桌 → 后厨加工间 → 出菜口
3. **追源动画** — 食客说"太咸"→ 红线从盘子沿流水线追回到旋钮 #347
4. **六种手法卡片** — 混合/调配/浓缩/提味/发酵/品控
5. **成长曲线** — 4 道菜照片：第1道（黑暗料理）→ 第1000道（精致摆盘）
6. **路边摊→米其林** — 同一个厨师，同一套手法，不同规模

## 写作原则
- **一个厨房，一个故事** — 从头到尾跟着小明，不跳出比喻
- **零公式** — 数学藏在调料背后
- **每章结尾一句话总结** — 让跳读的人也能抓住核心
- **不居高临下** — 读者是来参观厨房的客人，不是被教育的学生
- **Karpathy 是食材供应商** — 我们是翻译层，把原料变成成品菜
